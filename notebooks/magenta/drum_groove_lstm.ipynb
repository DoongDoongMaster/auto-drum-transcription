{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import music21\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFAULT_DRUM_TYPE_PITCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DRUM_TYPE_PITCHES = [\n",
    "    # kick drum\n",
    "    [36, 35],\n",
    "# 35: i.BassDrum,  # Acoustic Bass Drum\n",
    "# 36: i.BassDrum,  # Bass Drum 1\n",
    "\n",
    "    # snare drum\n",
    "    [38, 27, 28, 31, 32, 33, 34, 37, 39, 40, 56, 65, 66, 75, 85],\n",
    "# 37: i.SnareDrum,  # Side Stick\n",
    "# 38: i.SnareDrum,  # Acoustic Snare\n",
    "# 40: i.SnareDrum,  # Electric Snare\n",
    "# 56: i.Cowbell,\n",
    "\n",
    "    # closed hi-hat\n",
    "    [42, 44, 54, 68, 69, 70, 71, 73, 78, 80, 22],\n",
    "# 42: i.HiHatCymbal,  # Closed Hi Hat\n",
    "# 44: i.HiHatCymbal,  # Pedal Hi-Hat\n",
    "# 54: i.Tambourine,\n",
    "\n",
    "    # open hi-hat\n",
    "    [46, 67, 72, 74, 79, 81, 26],\n",
    "# 46: i.HiHatCymbal,  # Open Hi-Hat\n",
    "    # low tom\n",
    "    [45, 29, 41, 43, 61, 64, 84],\n",
    "# 41: i.TomTom,  # Low Floor Tom\n",
    "# 43: i.TomTom,  # High Floor Tom\n",
    "# 45: i.TomTom,  # Low Tom\n",
    "# 61: i.BongoDrums,  # Low Bongo\n",
    "# 64: i.CongaDrum,  # Low Conga\n",
    "\n",
    "    # mid tom\n",
    "    [48, 47, 60, 63, 77, 86, 87],\n",
    "# 47: i.TomTom,  # Low-Mid Tom\n",
    "# 48: i.TomTom,  # Hi-Mid Tom\n",
    "# 60: i.BongoDrums,  # Hi Bongo\n",
    "# 63: i.CongaDrum,  # Open Hi Conga\n",
    "\n",
    "    # high tom\n",
    "    [50, 30, 62, 76, 83],\n",
    "# 50: i.TomTom,  # High Tom\n",
    "# 62: i.CongaDrum,  # Mute Hi Conga\n",
    "\n",
    "\n",
    "    # crash cymbal\n",
    "    [49, 52, 55, 57, 58],\n",
    "# 49: i.CrashCymbals,  # Crash Cymbal 1\n",
    "# 57: i.CrashCymbals,  # Crash Cymbal 2\n",
    "# 58: i.Vibraslap,\n",
    "\n",
    "    # ride cymbal\n",
    "    [51, 53, 59, 82],\n",
    "\n",
    "\n",
    "    # Percussion\n",
    "]\n",
    "\n",
    "i = {\n",
    "    # 65: i.Timbales,  # High Timbale\n",
    "    # 66: i.Timbales,  # Low Timbale\n",
    "    # 67: i.Agogo,  # High Agogo\n",
    "    # 68: i.Agogo,  # Low Agogo\n",
    "    # # 69: i.Cabasa,\n",
    "    # 70: i.Maracas,\n",
    "    # 71: i.Whistle,  # Short Whistle\n",
    "    # 72: i.Whistle,  # Long Whistle\n",
    "    # # 73: i.Short Guiro,\n",
    "    # # 74: i.Long Guiro,\n",
    "    # # 75: i.Claves,\n",
    "    # 76: i.Woodblock,  # Hi Wood Block\n",
    "    # 77: i.Woodblock,  # Low Wood Block\n",
    "    # # 78: i.Mute Cuica,\n",
    "    # # 79: i.Open Cuica,\n",
    "    # 80: i.Triangle,  # Mute Triangle\n",
    "    # 81: i.Triangle,  # Open Triangle\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### midiPitchToInstrument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculet_time(midi_part, tempo_sec):\n",
    "    # midi.parts[i].flatten().notes\n",
    "    peak_t = [-1]\n",
    "    for parts in midi_part.parts:\n",
    "        # top = parts.flatten().notes\n",
    "        # print(len(midi_part.flatten()))\n",
    "        for nt in parts.flatten().notes:\n",
    "        # for nt in midi_part: \n",
    "            # 타악기.\n",
    "            if isinstance(nt, percussion.PercussionChord) or isinstance(nt, note.Unpitched):\n",
    "                if peak_t[-1] != nt.offset * tempo_sec:\n",
    "                    peak_t.append(nt.offset * tempo_sec)\n",
    "\n",
    "\n",
    "    return peak_t[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<music21.tempo.MetronomeMark Quarter=95>\n",
      "0.631578947368421 95.0\n",
      "<music21.meter.TimeSignature 4/4>\n"
     ]
    }
   ],
   "source": [
    "from music21 import *\n",
    "\n",
    "file = midi.MidiFile()\n",
    "file.open(\"../../data/tmp_groove/122_funk_95_fill_4-4.mid\")\n",
    "file.read()\n",
    "file.close()\n",
    "\n",
    "# -- 95 bpm, 4/4 박자에서 4분 음표의 길이 0.631 => tempo_sec * offset = 현재 초\n",
    "for e in file.tracks[0].events[:]:\n",
    "    if e.type == midi.MetaEvents.SET_TEMPO:\n",
    "        tempo_data = midi.translate.midiEventsToTempo(e)\n",
    "        print(tempo_data)\n",
    "        tempo_sec = tempo_data.secondsPerQuarter()\n",
    "        bpm = tempo_data.getQuarterBPM()\n",
    "        print(tempo_sec, bpm)\n",
    "    elif e.type == midi.MetaEvents.TIME_SIGNATURE:\n",
    "        print(midi.translate.midiEventsToTimeSignature(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Score' object has no attribute 'tracks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m             \u001b[39mprint\u001b[39m(midi\u001b[39m.\u001b[39mtranslate\u001b[39m.\u001b[39mmidiEventsToTimeSignature(e))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# for element in midi_stream.flat.notes:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m#     if isinstance(element, note.Note):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m#         # For pitched notes\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m             \u001b[39m# print(\"percussion >> \", element.quarterLength)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m analyze_drum_midi(\u001b[39m\"\u001b[39;49m\u001b[39m../../data/tmp_groove/122_funk_95_fill_4-4.mid\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m midi_stream \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39mparse(midi_file)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# midi_stream = midi.translate.midiFileToStream(file)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m midi_stream\u001b[39m.\u001b[39;49mtracks[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mevents[:\u001b[39m10\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m midi\u001b[39m.\u001b[39mMetaEvents\u001b[39m.\u001b[39mSET_TEMPO:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         tempo_data \u001b[39m=\u001b[39m midi\u001b[39m.\u001b[39mtranslate\u001b[39m.\u001b[39mmidiEventsToTempo(e)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Score' object has no attribute 'tracks'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from music21 import *\n",
    "\n",
    "pm = midi.percussion.PercussionMapper()\n",
    "\n",
    "def analyze_drum_midi(midi_file):\n",
    "    midi_stream = converter.parse(midi_file)\n",
    "    for element in midi_stream.flat.notes:\n",
    "        if isinstance(element, note.Note):\n",
    "            # For pitched notes\n",
    "            print(\"Offset:\", element.offset, \"| Pitch:\", element.pitch.midi, \"| Duration:\", element.duration.quarterLength)\n",
    "        elif isinstance(element, chord.Chord):\n",
    "            # For chords (potentially including PercussionChord)\n",
    "            pitches = [p.midi for p in element.pitches]\n",
    "            print(\"Offset:\", element.offset, \"| Pitches:\", pitches, \"| Duration:\", element.duration.quarterLength)\n",
    "        # elif isinstance(element, percussion.PercussionChord):\n",
    "        elif isinstance(element, note.Unpitched):\n",
    "            # if element.storedInstrument:\n",
    "            print(element.storedInstrument.percMapPitch)\n",
    "            # print(element.storedInstrument.instrumentName)\n",
    "        elif isinstance(element, percussion.PercussionChord):\n",
    "            allNotes = []\n",
    "            for thisNote in element.notes:\n",
    "                if isinstance(thisNote, note.Note):\n",
    "                    allNotes.append(thisNote.nameWithOctave)\n",
    "                elif isinstance(thisNote, note.Unpitched):\n",
    "                    if thisNote.storedInstrument:\n",
    "                        allNotes.append(thisNote.storedInstrument.percMapPitch)\n",
    "                        # allNotes.append(str(thisNote.storedInstrument.thisNote.storedInstrument))\n",
    "                    else:\n",
    "                        allNotes.append(f'unpitched[{thisNote.displayName}]')\n",
    "\n",
    "\n",
    "            print(\"percussion >> \", element.quarterLength)\n",
    "analyze_drum_midi(\"../../data/tmp_groove/122_funk_95_fill_4-4.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Audio Feature Extraction using librosa\n",
    "max_pad_len = 174\n",
    "def extract_audio_features(wav_file):\n",
    "    audio, sample_rate = librosa.load(wav_file)\n",
    "    # audio, sample_rate = librosa.load(file_name)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    pad_width = max_pad_len - mfccs.shape[1]\n",
    "    mfccs = np.pad(mfccs, pad_width=((0,0), (0, pad_width)), mode='constant')\n",
    "    \n",
    "    return mfccs.T  # Transpose for sequence length as the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: MIDI Information Extraction using music21\n",
    "import matplotlib.pyplot as plt\n",
    "from music21 import *\n",
    "\n",
    "def extract_midi_information(midi_file):\n",
    "    midi_stream = music21.converter.parse(midi_file)\n",
    "    \n",
    "    hits = []\n",
    "    # 스트림 평탄화: 중첩된 스트림을 평탄화 한다.\n",
    "    for element in midi_stream.flat.notes:\n",
    "        if isinstance(element, note.Unpitched):# -- unpitched : 한 개 침\n",
    "            hits.append({'offset' : float(element.offset),\n",
    "                         'quarterLength': float(element.quarterLength),\n",
    "                         'instrument': [element.storedInstrument.percMapPitch]})\n",
    "        elif isinstance(element, percussion.PercussionChord):# -- percussion : 동시에 여러 개 침\n",
    "            allNotes = []\n",
    "            for thisNote in element.notes:\n",
    "                if isinstance(thisNote, note.Note):\n",
    "                    allNotes.append(thisNote.nameWithOctave)\n",
    "                elif isinstance(thisNote, note.Unpitched):\n",
    "                    if thisNote.storedInstrument:\n",
    "                        allNotes.append(thisNote.storedInstrument.percMapPitch)\n",
    "                    # else:\n",
    "                    #     allNotes.append(f'unpitched[{thisNote.displayName}]')\n",
    "            hits.append({'offset' : float(element.offset),\n",
    "                'quarterLength': float(element.quarterLength),\n",
    "                'instrument': allNotes}) ## -- element.storedInstrument\n",
    "            \n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Data Preparation\n",
    "# Assuming you have a dataset with paired WAV and MIDI files\n",
    "# and you have labeled MIDI information for training\n",
    "def prepare_data(dataset_path):\n",
    "    X = []  # Input: Audio Features\n",
    "    y = []  # Output: MIDI Information (hit timing and drum type)\n",
    "\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            wav_path = os.path.join(dataset_path, filename)\n",
    "            midi_path = os.path.join(dataset_path, filename.replace(\".wav\", \".mid\"))\n",
    "\n",
    "            # Extract features from WAV file\n",
    "            audio_features = extract_audio_features(wav_path)\n",
    "            X.append(audio_features)\n",
    "\n",
    "            # Extract MIDI information\n",
    "            midi_information = extract_midi_information(midi_path)\n",
    "            y.append(midi_information)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Model Architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(174, 40), return_sequences=True))\n",
    "model.add(Dense(2)) # 2 outputs: hit timing and drum type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Music21Object._getTimeSignatureForBeat of <music21.percussion.PercussionChord [Hi-Hat Cymbal Snare Drum]>>\n",
      "<bound method Music21Object._getTimeSignatureForBeat of <music21.percussion.PercussionChord [Snare Drum Snare Drum]>>\n",
      "<bound method Music21Object._getTimeSignatureForBeat of <music21.percussion.PercussionChord [Tom-Tom Tom-Tom]>>\n",
      "<bound method Music21Object._getTimeSignatureForBeat of <music21.note.Unpitched object at 0x7f36cc533110>>\n",
      "<bound method Music21Object._getTimeSignatureForBeat of <music21.note.Unpitched object at 0x7f36cc532d10>>\n",
      "<bound method Music21Object._getTimeSignatureForBeat of <music21.percussion.PercussionChord [Snare Drum Snare Drum]>>\n",
      "<bound method Music21Object._getTimeSignatureForBeat of <music21.percussion.PercussionChord [Tom-Tom Tom-Tom]>>\n",
      "<bound method Music21Object._getTimeSignatureForBeat of <music21.note.Unpitched object at 0x7f36cc5a52d0>>\n",
      "<bound method Music21Object._getTimeSignatureForBeat of <music21.note.Unpitched object at 0x7f36cc5a5550>>\n",
      "[0.0 0.25 list([44, 40])]\n",
      "[0.5 0.25 list([38, 38])]\n",
      "[0.6666666666666666 0.3333333333333333 list([48, 43])]\n",
      "[1.0 0.25 list([36])]\n",
      "[1.5 0.25 list([38])]\n",
      "[2.0 0.25 list([38, 38])]\n",
      "[2.25 0.25 list([48, 43])]\n",
      "[2.5 0.25 list([36])]\n",
      "[3.0 0.25 list([40])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_325/2785238704.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  labels_array = np.array([data['offset'], data['quarterLength'], data['instrument']])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X, y = prepare_data(\"../../data/tmp_groove/\")\n",
    "y_train = []\n",
    "for y_data in y:\n",
    "    tmp = []\n",
    "    for data in y_data:\n",
    "        labels_array = np.array([data['offset'], data['quarterLength'], data['instrument']])\n",
    "        print(labels_array)\n",
    "    y_train.append(tmp)\n",
    "\n",
    "n_columns = 174    \n",
    "n_row = 40       \n",
    "n_channels = 1\n",
    "n_classes = 4\n",
    "X_train = np.array(X)\n",
    "X_train = tf.reshape(X_train, [-1, n_row, n_columns, n_channels])\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Step 5: Training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Adjust loss function as needed\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/magenta/drum_groove_lstm.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1795\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1792\u001b[0m split_at \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mfloor(batch_dim \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m validation_split)))\n\u001b[1;32m   1794\u001b[0m \u001b[39mif\u001b[39;00m split_at \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m split_at \u001b[39m==\u001b[39m batch_dim:\n\u001b[0;32m-> 1795\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1796\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining data contains \u001b[39m\u001b[39m{batch_dim}\u001b[39;00m\u001b[39m samples, which is not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1797\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msufficient to split it into a validation and training set as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1798\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecified by `validation_split=\u001b[39m\u001b[39m{validation_split}\u001b[39;00m\u001b[39m`. Either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1799\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide more data, or a different value for the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1800\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`validation_split` argument.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1801\u001b[0m             batch_dim\u001b[39m=\u001b[39mbatch_dim, validation_split\u001b[39m=\u001b[39mvalidation_split\n\u001b[1;32m   1802\u001b[0m         )\n\u001b[1;32m   1803\u001b[0m     )\n\u001b[1;32m   1805\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split\u001b[39m(t, start, end):\n\u001b[1;32m   1806\u001b[0m     \u001b[39mif\u001b[39;00m t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Training data contains 1 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "# Step 5: Training\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')  # Adjust loss function as needed\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_notes(midi_part):\n",
    "    parent_element = []\n",
    "    ret = []\n",
    "    print(f\"extract: {len(midi_part.stream().flatten().notes)}\")\n",
    "    # print(len(midi_part.flatten()))\n",
    "    for nt in midi_part.stream().flatten().notes:\n",
    "    # for nt in midi_part: \n",
    "        # 타악기.\n",
    "        if isinstance(nt, percussion.PercussionChord):\n",
    "            allNotes = []\n",
    "            for thisNote in nt.notes:\n",
    "                if isinstance(thisNote, note.Note):\n",
    "                    allNotes.append(thisNote.nameWithOctave)\n",
    "                elif isinstance(thisNote, note.Unpitched): # 드럼의 경우 unpictched 인듯. <- 확인이 필요함.\n",
    "                    if thisNote.storedInstrument:\n",
    "                        allNotes.append(str(thisNote.storedInstrument.instrumentName))\n",
    "                    else:\n",
    "                        allNotes.append(f'unpitched[{thisNote.displayName}]')\n",
    "            ret.append(max(0.0, 0 if allNotes[-1] not in temp else temp[allNotes[-1]]))\n",
    "            parent_element.append(nt)\n",
    "            \n",
    "            print(allNotes, nt.duration.quarterLength, nt.offset, nt.seconds, nt.beat)\n",
    "        elif isinstance(nt, note.Unpitched): # 드럼의 경우 unpictched 인듯. <- 확인이 필요함.\n",
    "            if nt.storedInstrument:\n",
    "                this_note = str(nt.storedInstrument.instrumentName)\n",
    "            else:\n",
    "                this_note = f'unpitched[{nt.displayName}]'\n",
    "            ret.append(max(0.0, 0 if this_note not in temp else temp[this_note]))\n",
    "            parent_element.append(nt)\n",
    "            print(this_note, nt.duration.quarterLength, nt.offset, nt.seconds, nt.beat)\n",
    "        else:\n",
    "            print(nt)\n",
    "    \n",
    "    return ret, parent_element"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
