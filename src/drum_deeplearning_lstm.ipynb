{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../data/tmp_wavs/\" # ÎìúÎüº ÎÖπÏùåÎ≥∏ path\n",
    "endswith = {\"m4a\":\".m4a\", \"wav\":\".wav\"} # ÌôïÏû•ÏûêÎ™Ö\n",
    "ORIGINAL_AUDIO_EXT = 'm4a'\n",
    "AUDIO_EXT = 'wav'\n",
    "\n",
    "# sr(sampling rate) : default 22050\n",
    "# -> Ïò§ÎîîÏò§ Ï≤òÎ¶¨Ïùò ÏùºÎ∞òÏ†ÅÏù∏ Í¥ÄÌñâÏúºÎ°ú ÌäπÌûà ÏùåÏïÖ Ï†úÏûë ÌôòÍ≤ΩÏóêÏÑúÎäî 44.1kHzÏùò ÏÉòÌîå ÏÜçÎèÑÍ∞Ä ÏûêÏ£º ÏÇ¨Ïö©Îê©ÎãàÎã§. Í∑∏Îü¨ÎÇò ÏùºÎ∂Ä ÌäπÏ†ï ÏûëÏóÖÏùò Í≤ΩÏö∞ ÎòêÎäî Ï†úÌïúÎêú Î¶¨ÏÜåÏä§Î•º Ï≤òÎ¶¨Ìï† ÎïåÎäî 22.05kHz ÎòêÎäî 16kHzÏôÄ Í∞ôÏùÄ ÎÇÆÏùÄ ÏÉòÌîåÎßÅ ÏÜçÎèÑÎ°úÎèÑ Ï∂©Î∂Ñ\n",
    "SAMPLE_RATE = 22050\n",
    "\n",
    "def get_audios(ext):\n",
    "    file_list = os.listdir(root_path)\n",
    "    files = [os.path.join(root_path, file) for file in file_list if file.endswith(endswith[ext])]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚û°Ô∏è m4a to wav (ÌïÑÏöîÏãú)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT_VIDEO_COMMAND = ('ffmpeg -i \"{from_video_path}\" '\n",
    "#                          '-f {audio_ext} -ab 22500 '\n",
    "#                          '-vn \"{to_audio_path}\" ')\n",
    "\n",
    "# files = get_audios(ORIGINAL_AUDIO_EXT)\n",
    "# for file in files:\n",
    "#     audio_file_name = file.replace(ORIGINAL_AUDIO_EXT, AUDIO_EXT)\n",
    "#     print(audio_file_name)\n",
    "#     command = EXTRACT_VIDEO_COMMAND.format(\n",
    "#         from_video_path=file, audio_ext=AUDIO_EXT, to_audio_path=audio_file_name,\n",
    "#     )\n",
    "#     os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚û°Ô∏è train data Í∞ÄÏ†∏Ïò§Í∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_audios(AUDIO_EXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéµ ÏΩîÎìú ÏÇ¨Ï†Ñ Ï†ïÏùò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2idx = {'CC_04':0, 'CC_08':1,\n",
    "             'HH_04':2, 'HH_08':3, 'HH_16':4, \n",
    "             'KK_04':5, 'KK_08':6,\n",
    "            'SD_04':7, 'SD_08':8}\n",
    "\n",
    "idx2code = {0:'CC_04', 1:'CC_08',\n",
    "             2:'HH_04', 3:'HH_08', 4:'HH_16', \n",
    "             5:'KK_04', 6:'KK_08',\n",
    "            7:'SD_04', 8:'SD_08'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data Ï†ÑÏ≤òÎ¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pad_len = 800\n",
    "\n",
    "def extract_feature(file):\n",
    "    audio, sample_rate = librosa.load(file)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    pad_width = max_pad_len - mfccs.shape[1]\n",
    "    mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"CC_08\" : [\"CC_08\"],\n",
    "    \"HH_04\" : [\"HH_04\", \"HH_04\", \"HH_04\", \"HH_04\"],\n",
    "    \"HH_08\" : [\"HH_08\", \"HH_08\", \"HH_08\", \"HH_08\", \"HH_08\", \"HH_08\", \"HH_08\", \"HH_08\"],\n",
    "    \"HH_16\" : [\"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\", \"HH_16\"],\n",
    "    \"KK_04\" : [\"KK_04\", \"KK_04\", \"KK_04\", \"KK_04\"],\n",
    "    \"KK_08\" : [\"KK_08\", \"KK_08\", \"KK_08\", \"KK_08\", \"KK_08\", \"KK_08\", \"KK_08\", \"KK_08\"],\n",
    "    \"SD_04\" : [\"SD_04\", \"SD_04\", \"SD_04\", \"SD_04\"],\n",
    "    \"SD_08\" : [\"SD_08\", \"SD_08\", \"SD_08\", \"SD_08\", \"SD_08\", \"SD_08\", \"SD_08\", \"SD_08\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for file in files:\n",
    "    data = extract_feature(file)\n",
    "    file_name = file.replace(root_path, \"\")\n",
    "    class_label = output[file_name[0:5]]\n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "X_train = np.array(featuresdf.feature.tolist())\n",
    "y_train_class = np.array(featuresdf.class_label)\n",
    "y_train = []\n",
    "\n",
    "for y in y_train_class:\n",
    "    tmp = []\n",
    "    for idx in y:\n",
    "        tmp.append(code2idx[idx])\n",
    "    y_train.append(tmp)\n",
    "\n",
    "X_train = np.array(X_train)  # Convert to NumPy array\n",
    "X_train_transposed = np.transpose(X_train, (0, 2, 1))\n",
    "\n",
    "# Assuming y_train is a list of lists\n",
    "y_train_padded = pad_sequences(y_train, padding='post', maxlen=max_pad_len, value=-1)\n",
    "\n",
    "# Convert to NumPy array\n",
    "y_train = np.array(y_train_padded)\n",
    "\n",
    "# One-hot encode y_train\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=9)  # Adjust the number of classes as needed\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# yy = to_categorical(le.fit_transform(y_train))\n",
    "# print(\"X_train >>> \", X_train)\n",
    "# print(\"y_train >>> \", len(y_train[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 800, 40)]         0         \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 798, 64)           7744      \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 798, 64)           33024     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 798, 9)            585       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41353 (161.54 KB)\n",
      "Trainable params: 41353 (161.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the input shape based on your audio features\n",
    "input_shape = (max_pad_len, 40)\n",
    "\n",
    "input_layer = layers.Input(shape=input_shape)\n",
    "conv1d_layer = layers.Conv1D(64, kernel_size=3, activation='relu')(input_layer)\n",
    "lstm_layer = layers.LSTM(64, return_sequences=True)(conv1d_layer)\n",
    "# output_layer = layers.Dense(1, activation='linear')(lstm_layer)\n",
    "# output_layer = layers.Dense(len(code2idx), activation='softmax')(lstm_layer)\n",
    "output_layer = Dense(9, activation='softmax')(lstm_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 800, 9) and (None, 798, 9) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/drum_deeplearning_lstm.ipynb Cell 17\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/drum_deeplearning_lstm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/drum_deeplearning_lstm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# X_train is your padded/truncated input data (WAV files), y_train is your output data (drum hits)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/wotjr/Documents/Github/drum-classification/src/drum_deeplearning_lstm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_transposed, y_train_one_hot, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filenlr3xt1b.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/jaeserrr/anaconda3/envs/drum/lib/python3.11/site-packages/keras/src/backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 800, 9) and (None, 798, 9) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# X_train is your padded/truncated input data (WAV files), y_train is your output data (drum hits)\n",
    "model.fit(X_train_transposed, y_train_one_hot, epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
