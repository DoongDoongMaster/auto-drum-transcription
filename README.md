# Automatic Drum Transcription (ADT)
>ë“œëŸ¼ì˜ ë°•ìë¥¼ ì¸ì‹í•˜ê³  ì•…ê¸°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë“œëŸ¼ ì „ì‚¬ ëª¨ë¸

## ğŸ“ Table of Contents
- [Introduction](https://github.com/DoongDoongMaster/automatic-drum-transcription/edit/main/README.md#introduction)
- [Dataset](https://github.com/DoongDoongMaster/automatic-drum-transcription/edit/main/README.md#dataset)
- [Preparation](https://github.com/DoongDoongMaster/automatic-drum-transcription/edit/main/README.md#preparation)
- [Run](https://github.com/DoongDoongMaster/automatic-drum-transcription/edit/main/README.md#run)
<br>

## Introduction
### ğŸ¥ADT ë€?
ë“œëŸ¼ ì˜¤ë””ì˜¤ ì‹ í˜¸ì—ì„œ ìŒì•…ì  ê¸°í˜¸ë¥¼ ì¶”ì¶œí•´, ì•…ë³´ í˜•íƒœë¡œ ì¬êµ¬ì„±í•˜ëŠ” ê²ƒ

### ğŸ‘©â€ğŸ”¬ì—°êµ¬ ë°©ë²•
<img src = "https://github.com/DoongDoongMaster/automatic-drum-transcription/assets/68186101/5218b993-55e3-41a5-b525-4130c36165e7" width="100%" height="100%">

#### Segment and Classify
: ë“œëŸ¼ ì˜¤ë””ì˜¤ ì‹ í˜¸ ê°•ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Onset ì„ ì¸ì‹í•˜ì—¬ ì˜¤ë””ì˜¤ ì¡°ê°ìœ¼ë¡œ ìë¥´ê³ , ê° ì¡°ê°ì˜ íŠ¹ì„±ì—ì„œ íŒ¨í„´ì„ ì¸ì‹

#### Separate and Detect
: ì˜¤ë””ì˜¤ ì‹ í˜¸ë¥¼ ë“œëŸ¼ ì•…ê¸°ì˜ íŠ¹ì„±ì— ë”°ë¼ ë¶„ë¥˜í•˜ê³ , ê° ì•…ê¸°ì— ëŒ€í•œ í™œì„±í™” í•¨ìˆ˜ë¡œë¶€í„° Peak-Picking ì„ ê±°ì³ Onset ì„ ì¶”ì¶œ

### â“Why
ë“œëŸ¼ ì „ì‚¬ ëª¨ë¸ì„ í†µí•´ ì‚¬ìš©ìì˜ ë“œëŸ¼ ì—°ì£¼ë¥¼ ì¸ì‹í•´ì„œ, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ë‹µ ì•…ë³´ì™€ ë¹„êµí•´ ì±„ì ì„ í•˜ëŠ” ê¸°ëŠ¥ì„ ìœ„í•´ í•„ìš”í•œ ê¸°ìˆ 
<br><br>

## Dataset
- ADT ì—°êµ¬ ë¶„ì•¼ì—ì„œ ë³´í¸ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì…‹ ì‚¬ìš©
- ì–´ì¿ ìŠ¤í‹± ë“œëŸ¼ì…‹ì— ë§ì¶° clean ê³¼ì •ì„ ê±°ì¹¨
- ì˜¨ì…‹ì„ ê¸°ì¤€ìœ¼ë¡œ ë™ì‹œì— ì¹œ ìŒì„ íŒë‹¨ í›„, ë‹¤ìŒ ì˜¨ì…‹ì´ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€ ìŒì„ ì˜ë¼ì„œ í† í°ìœ¼ë¡œ ë§Œë“œëŠ” ê³¼ì •ì„ ê±°ì¹¨

|ë°ì´í„°ì…‹|ê¸¸ì´|í† í° ê°œìˆ˜|
|------|---|---|
|[MDB](https://github.com/CarlSouthall/MDBDrums)|35 m|3,638 ê°œ|
|[IDMT](https://www.idmt.fraunhofer.de/en/publications/datasets/drums.html)|116 m|10,489 ê°œ|
|[E-GMD](https://magenta.tensorflow.org/datasets/e-gmd)|26,374 m|126,608 ê°œ|
|[ENST](https://perso.telecom-paristech.fr/grichard/ENST-drums/)|194 m|23,503 ê°œ|

<br><br>

## Preparation
1. ë°ì´í„°ì…‹ ì•„ë˜ í´ë” ê²½ë¡œë¡œ ë‹¤ìš´ë¡œë“œ
  ```
  automatic-drum-transcription
  â”œâ”€â”€ data
  â”‚   â”œâ”€â”€ raw
  â”‚   â”‚   â”œâ”€â”€ MDBDrums
  â”‚   â”‚   â”œâ”€â”€ IDMT-SMT-DRUMS-V2
  â”‚   â”‚   â”œâ”€â”€ e-gmd-v1.0.0
  â”‚   â”‚   â”œâ”€â”€ ENST-drums-public
  ```
2. create conda env
  ```shell
  conda env create -f drum.yml
  ```

<br><br>

## Run
### 1. Feature Extraction
- ì˜¤ë””ì˜¤ ì¡°ê°ìœ¼ë¡œ ìë¥¸ í›„, `Mel-Spectrogram` ìœ¼ë¡œ í”¼ì³ ì¶”ì¶œ
- ì‹¤í–‰ ì½”ë“œ (`src/` ìœ„ì¹˜ì—ì„œ ì‹¤í–‰)
  ```python
  data_paths = DataProcessing.get_paths({ë°ì´í„° ê²½ë¡œ})
  FeatureExtractor.feature_extractor(data_paths, METHOD_DETECT, MEL_SPECTROGRAM, PKL)
  FeatureExtractor.load_feature_file(METHOD_DETECT, MEL_SPECTROGRAM, PKL, ENST, TRAIN)
  ```
### 2. Training
- `Segment and Classify`, `Separate and Detect` ì¤‘ ì›í•˜ëŠ” ëª¨ë¸ í´ë˜ìŠ¤ ì„ íƒí•´ì„œ í•™ìŠµ ì§„í–‰ ê°€ëŠ¥ 
- ì‹¤í–‰ ì½”ë“œ (`src/` ìœ„ì¹˜ì—ì„œ ì‹¤í–‰)
  ```python
  # == split_data, label_type ë§¤ê°œë³€ìˆ˜ ë°”ê¿”ì„œ ì‚¬ìš©!
  split_data = {
      TRAIN: [MDB, IDMT, ENST, E_GMD],
      VALIDATION: [MDB, IDMT, ENST, E_GMD],
      TEST: [MDB, IDMT, ENST, E_GMD],
  }

  # ëª¨ë¸ ê°ì²´ ìƒì„±
  segment_classify = SegmentClassifyModel(
      training_epochs=50,
      batch_size=8,
      opt_learning_rate=0.001,
      feature_type=MEL_SPECTROGRAM,
  )
  # split dataset
  segment_classify.create_dataset(split_data, group_dict=CLASSIFY_TYPES)
  # create model
  segment_classify.create()
  # train model
  segment_classify.train()
  ```
### 3. Evaluate
- ì‹¤í–‰ ì½”ë“œ (`src/` ìœ„ì¹˜ì—ì„œ ì‹¤í–‰)
  ```python
  segment_classify.evaluate() # ëª¨ë¸ í‰ê°€
  segment_classify.save() # ëª¨ë¸ ì €ì¥
  ```
### 4. Inferenct
- ì‹¤í–‰ ì½”ë“œ (`src/` ìœ„ì¹˜ì—ì„œ ì‹¤í–‰)
  ```python
  segment_classify = SegmentClassifyModel(feature_type=MEL_SPECTROGRAM)
  segment_classify.predict({ì˜ˆì¸¡í•  wav íŒŒì¼ ê²½ë¡œ}, {bpm ì •ë³´}, 0)
  ```

