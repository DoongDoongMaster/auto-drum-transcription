{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow_datasets\n",
    "# pip install magenta\n",
    "# pip install music21\n",
    "# pip install basic-pitch\n",
    "# pip install aubio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, corpus, instrument, midi, note, chord, pitch\n",
    "\n",
    "from music21 import percussion\n",
    "from music21.midi.percussion import MIDIPercussionException, PercussionMapper\n",
    "\n",
    "PERCUSSION_MAPPER = PercussionMapper()\n",
    "\n",
    "def open_midi(midi_path):\n",
    "    mf = midi.MidiFile()\n",
    "    mf.open(midi_path)\n",
    "    mf.read()\n",
    "    mf.close()       \n",
    "    print(len(mf.tracks)) # -- 1\n",
    "    return midi.translate.midiFileToStream(mf)\n",
    "\n",
    "midi_data = open_midi(\"../../data/tmp_groove/1_funk-groove1_138_beat_4-4.mid\")\n",
    "# midi_data # -- <music21.stream.Score ...>\n",
    "print(len(midi_data.flatten().notesAndRests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_notes(midi_part):\n",
    "#     parent_element = []\n",
    "#     ret = []\n",
    "#     for nt in midi_part.flat.notes:        \n",
    "#         if isinstance(nt, note.Note):\n",
    "#             ret.append(max(0.0, nt.pitch.ps))\n",
    "#             parent_element.append(nt)\n",
    "#         elif isinstance(nt, chord.Chord):\n",
    "#             for pitch in nt.pitches:\n",
    "#                 ret.append(max(0.0, pitch.ps))\n",
    "#                 parent_element.append(nt)\n",
    "    \n",
    "#     return ret, parent_element\n",
    "from music21 import stream\n",
    "\n",
    "temp_midi = stream.Score()\n",
    "temp_midi.insert(0, temp_midi_chords)\n",
    "\n",
    "notes = midi_data.measures(0, 1).parts.flat.notes\n",
    "midi_data.measures(0, 1).show(\"text\")\n",
    "print(len(notes))\n",
    "for nt in notes:\n",
    "    # print(nt)\n",
    "    print(isinstance(nt, chord.Chord))\n",
    "    # print(nt.pitch)\n",
    "# top = base_midi.measures(10, 80).parts.flat.notes.pitch              \n",
    "# y, parent_element = extract_notes(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(nr, note.Unpitched):\n",
    "    try:\n",
    "        i = PERCUSSION_MAPPER.midiPitchToInstrument(eOn.pitch)\n",
    "    except MIDIPercussionException:\n",
    "        # warnings.warn(str(mpe), TranslateWarning)\n",
    "        i = instrument.UnpitchedPercussion()\n",
    "    nr.storedInstrument = i\n",
    "    # TODO: set reasonable displayPitch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_instruments(midi):\n",
    "    partStream = midi.parts.stream()\n",
    "    print(\"List of instruments found on MIDI file:\")\n",
    "    for p in partStream:\n",
    "        aux = p\n",
    "        print (p.partName)\n",
    "\n",
    "list_instruments(base_midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "def extract_notes(midi_part):\n",
    "    parent_element = []\n",
    "    ret = []\n",
    "    for nt in midi_part.flat.notes:        \n",
    "        if isinstance(nt, note.Note):\n",
    "            ret.append(max(0.0, nt.pitch.ps))\n",
    "            parent_element.append(nt)\n",
    "        elif isinstance(nt, chord.Chord):\n",
    "            for pitch in nt.pitches:\n",
    "                ret.append(max(0.0, pitch.ps))\n",
    "                parent_element.append(nt)\n",
    "    \n",
    "    return ret, parent_element\n",
    "\n",
    "def print_parts_countour(midi):\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    minPitch = pitch.Pitch('C10').ps\n",
    "    maxPitch = 0\n",
    "    xMax = 0\n",
    "    \n",
    "    # Drawing notes.\n",
    "    for i in range(len(midi.parts)):\n",
    "        top = midi.parts[i].flat.notes                  \n",
    "        y, parent_element = extract_notes(top)\n",
    "        if (len(y) < 1): continue\n",
    "            \n",
    "        x = [n.offset for n in parent_element]\n",
    "        ax.scatter(x, y, alpha=0.6, s=7)\n",
    "        \n",
    "        aux = min(y)\n",
    "        if (aux < minPitch): minPitch = aux\n",
    "            \n",
    "        aux = max(y)\n",
    "        if (aux > maxPitch): maxPitch = aux\n",
    "            \n",
    "        aux = max(x)\n",
    "        if (aux > xMax): xMax = aux\n",
    "    \n",
    "    for i in range(1, 10):\n",
    "        linePitch = pitch.Pitch('C{0}'.format(i)).ps\n",
    "        if (linePitch > minPitch and linePitch < maxPitch):\n",
    "            ax.add_line(mlines.Line2D([0, xMax], [linePitch, linePitch], color='red', alpha=0.1))            \n",
    "\n",
    "    plt.ylabel(\"Note index (each octave has 12 notes)\")\n",
    "    plt.xlabel(\"Number of quarter notes (beats)\")\n",
    "    plt.title('Voices motion approximation, each color is a different instrument, red lines show each octave')\n",
    "    plt.show()\n",
    "\n",
    "# Focusing only on 6 first measures to make it easier to understand.\n",
    "print_parts_countour(base_midi.measures(0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tfds works in both Eager and Graph modes\n",
    "# # tf.enable_eager_execution()\n",
    "# # print(tf.executing_eagerly()) # -- True\n",
    "\n",
    "# # Load the full GMD with MIDI only (no audio) as a tf.data.Dataset\n",
    "# dataset = tfds.load(\n",
    "#     name=\"groove/full-midionly\",\n",
    "#     split=tfds.Split.TRAIN,\n",
    "#     try_gcs=True)\n",
    "\n",
    "# \"\"\"\n",
    "# Split\t        Examples\n",
    "# 'test'\t        129\n",
    "# 'train'\t        897\n",
    "# 'validation'\t124\n",
    "\n",
    "# FeaturesDict({\n",
    "#     'bpm': int32,\n",
    "#     'drummer': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
    "#     'id': string,\n",
    "#     'midi': string,\n",
    "#     'style': FeaturesDict({\n",
    "#         'primary': ClassLabel(shape=(), dtype=int64, num_classes=18),\n",
    "#         'secondary': string,\n",
    "#     }),\n",
    "#     'time_signature': ClassLabel(shape=(), dtype=int64, num_classes=5),\n",
    "#     'type': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
    "# })\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(dataset)\n",
    "# # # Build your input pipeline\n",
    "# # dataset = dataset.shuffle(1024).batch(32).prefetch(\n",
    "# #     tf.data.experimental.AUTOTUNE)\n",
    "# for features in dataset.take(1):\n",
    "#   # Access the features you are interested in\n",
    "#   midi, genre = features[\"midi\"], features[\"style\"][\"primary\"]\n",
    "#   print(midi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
